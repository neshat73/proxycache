
<img width="1000"  alt="image_" src="https://github.com/user-attachments/assets/a7e0e1d1-38c3-44fa-84e1-e7764ada3131" />


### Описание проекта

Сервис представляет собой «умный» прокси-сервер для нейросетевого движка llama.cpp, делающий работу с большими текстами и чат-ботами намного быстрее и эффективнее.

#### Для чего он нужен?

В llama.cpp есть замечательная функция — слоты. Каждый слот — это как отдельная «комната» для диалога, где хранится его контекст (KV-кэш). Если вы возвращаетесь в тот же слот с похожим диалогом, движок не пересчитывает всё заново, а продолжает с места совпадения. Это экономит массу времени, особенно при работе с большими контекстами, как в IDE, где промпт может достигать 30-60 тысяч токенов. Обработка такого промпта «с нуля» при скорости 500 токенов в секунду займёт 2 минуты, а подгрузка готового кэша — всего несколько секунд.

Проблема в том, что слотов обычно мало (например, 4), а пользователей — много (скажем, 20 программистов в команде). Если просто отправлять запросы, они будут случайным образом занимать слоты, перетирая чужие «горячие» контексты.

**Этот сервис решает данную проблему:** он выступает в роли диспетчера, который грамотно распределяет запросы по слотам, сохраняет неиспользуемые кэши на диск и подгружает их обратно, когда они снова нужны.

### Как работает балансировка и управление слотами

1. **Слоты: «горячие» и «холодные».** Каждый слот на сервере может быть «горячим» — то есть содержать полезный, свежий кэш, готовый к повторному использованию. Если запрос не подходит для этого кэша, слот помечается как «холодный».
2. **Выбор слота для нового запроса.** Когда приходит новый запрос, сервис:
    * Сравнивает его со всеми «горячими» слотами. Если находится очень похожий (например, на 85% и больше), запрос отправляется в этот слот. Это самый быстрый путь.
    * Если похожих «горячих» слотов нет, сервис ищет свободный слот. Если он есть, запрос отправляется туда, и этот слот становится «горячим».
    * Если свободных слотов нет, но есть «холодные», запрос занимает один из них.
3. **Сохранение кэша на диск.** Когда все слоты заняты и нужно место для нового запроса, сервис не просто удаляет старый кэш. Он выбирает самый «древний» (давно не используемый) слот, сохраняет его содержимое (KV-кэш) на диск в виде специального файла, и только потом освобождает слот.
4. **Восстановление кэша с диска.** Если приходит запрос, для которого на диске есть сохранённый кэш, сервис делает следующее:
    * Находит свободный, «холодный» или самый старый «горячий» слот.
    * Загружает в него нужный кэш с диска (это занимает всего несколько секунд).
    * Отправляет запрос в этот подготовленный слот, который теперь снова «горячий».

Таким образом, даже при ограниченном числе слотов, сервис эффективно управляет десятками и сотнями уникальных контекстов, значительно ускоряя работу для всех пользователей.

### Инструкция по запуску 

#### Шаг 1: Запуск нейросетевого движка (llama.cpp) https://github.com/ggml-org/llama.cpp

Сначала нужно запустить сам сервер llama.cpp, указав ему, сколько «комнат»-слотов создать и где хранить файлы кэшей.

```bash
./llama-server -m ваша-модель.gguf -np 4 --slot-save-path /путь/к/папке/кэшей
```

- `ваша-модель.gguf` — файл нейросети.
- `-np 4` — создать 4 слота.
- `--slot-save-path /путь/к/папке/кэшей` — папка, где сервер будет хранить файлы кэшей. **Это обязательно!**

если возникают проблемы работы gpt-oss-20b с IDE типа cline , воспользуйтесь данной инструкцией https://www.reddit.com/r/CLine/comments/1mtcj2v/making_gptoss_20b_and_cline_work_together/



#### Шаг 2: Настройка и запуск прокси-сервиса

1. **Скачайте файлы прокси-сервиса** и перейдите в папку с ними.
2. **Установите зависимости** (программы, от которых зависит работа сервиса):

```bash
python3 -m venv venv && source venv/bin/activate && pip install -r requirements.txt

```

3. **Настройте параметры** (самое важное — указать адрес запущенного движка):
    - Откройте файл `config.py`.
    - Найдите строку `LLAMA_SERVER_URL` и убедитесь, что там указан правильный адрес вашего llama.cpp сервера (обычно `http://127.0.0.1:8080`).
    - Убедитесь, что `SLOTS_COUNT` соответствует числу слотов, указанному при запуске сервера (`-np 4` в нашем примере).
4. **Запустите прокси-сервис:**

```bash
python3 proxycache.py  # or: uvicorn app:app --host 0.0.0.0 --port 8081
```


Теперь все запросы нужно отправлять не напрямую в llama.cpp, а на адрес прокси-сервиса (обычно `http://127.0.0.1:8081`). Он сам разберётся, в какой слот направить запрос, что сохранить на диск и что подгрузить обратно.

### Описание параметров 

- `LLAMA_SERVER_URL`: Адрес, где работает ваш нейросетевой движок.
- `SLOTS_COUNT`: Сколько «комнат»-слотов вы выделили для диалогов.
- `SIMILARITY_MIN_RATIO`: Насколько (в процентах) новый запрос должен быть похож на содержимое «горячего» слота, чтобы его можно было использовать. Например, `0.85` означает 85% схожести. Это защищает полезные кэши от случайного перетирания.
- `MIN_PREFIX_CHARS`: Если запрос очень короткий (меньше этого числа символов), он считается «маленьким» и отправляется в «холодный» или самый старый слот, чтобы не занимать ценные «горячие» слоты.
- `LOCAL_META_DIR`: Папка, где прокси хранит свою «картотеку» — небольшие файлы с описанием того, какой кэш где лежит. Сами кэши хранятся там, где вы указали в `--slot-save-path`.
